# 使用逻辑卷创建 BlueStore OSD

## BlueStore 简介

BlueStore 取代 FileStore 作为 OSD 的存储后端。FileStore 现已弃用

FileStore 将对象存储为块设备基础上的文件系统（通常是 XFS）中的文件。BlueStore 将对象直接存储在原始块设备上，免除了对文件系统层的需要，从而提高了读写操作速度

### BlueStore 架构

Ceph 集群中存储的对象具有集群范围的唯一标识符、二进制对象数据和对象元数据，BlueStore 将 对象元数据存储在块数据库中，块数据库将元数据作为键值对存储在 RocksDB 数据库中，这是一种高性能的键值存储，块数据库驻留于存储设备上的一个小型 BlueFS 分区，BlueFS 是一种最小的文件系统，设计用于保
存 RocksDB 文件，BlueStore 利用预写式日志 (WAL) 以原子式式将数据写入到块设备。预写式日志执行日志记录功能，并记录所有事务

![](https://gitee.com/cnlxh/ceph/raw/master/images/component/component-bluestore-ceph-osd.svg)

### BlueStore 性能

FileStore 写入到日志，然后从日志中写入到块设备。BlueStore 可避免这种双重写入的性能损失，直接将数据写入块设备，同时使用单独的数据流将事务记录到预写式日志，当工作负载相似 时，BlueStore 的写操作速度约为 FileStore 的两倍，如果在集群中混用不同的存储设备，您可以自定义 BlueStore OSD 来提入性能。创建新的 BlueStore OSD 时，默认为将数据、块数据库和预写式日志都放置到同一个块设备上。从数据中分离块数据库和预写式日志，并将它们放入更快速的 SSD 或 NVMe 设备，或许能提高性能。

如果将块数据库或预写式日志放置到与对象数据不同的存储设备上，或许能够提升性能，但条件是这个设备的速度要快于主要存储设备。例如，如果对象数据位于 HDD设备上，可以通过将块数据库放在 SSD 设备上并将预写式日志放到 NVMe 设备上来提高性能

使用服务规范文件定义BlueStore数据、块数据库和预写日志设备的位置。示例如下：指定OSD服务对应的BlueStore设备

```yaml
service_type: osd
service_ id: osd_example
placement:
  host_pattern: '*'
data_devices:
  paths:
    - /dev/vda
db_ devices: 
  paths: 
    - /dev/nvme0
wal_devices:
  paths: 
    - /dev/nvme1
```

BlueStore 存储后端提供下列功能：

1. 允许将不同的设备用于数据、块数据库和预写式日志 (WAL)

2. 支持以虚拟方式使用HDD、SSD 和 NVMe 设备的任意组合

3. 通过提高元数据效率，可以消除对存储设备的双重写入

以下图表显示了 BlueStore 与较旧 FileStore 解决方案的性能对比

![](https://gitee.com/cnlxh/ceph/raw/master/images/component/component-radosbench-write.svg)

![](https://gitee.com/cnlxh/ceph/raw/master/images/component/component-radosbench-read.svg)

BlueStore 在用户空间内运行，管理自己的缓存，并且其内存占用比FileStore 少。如有需要，可以手动调优 BlueStore 参数，BlueStore使用RocksDB存储键值元数据，BlueStore默认是自调优，但如果需要，可以手动调优BlueStore参数，

BlueStore分区写数据的块大小为bluestore_ min_alloc_size参数的大小，缺省值为4kib，如果要写入的数据小于chunk的大小，则BlueStore将chunk的剩余空间用0填充，建议将该参数设置为裸分区上最小的典型写操作的大小，建议将FileStore osd重新创建为BlueStore，以利用性能改进并维护红帽支持

### 介绍BlueStore数据库分片

BlueStore可以限制存储在RocksDB中的大型map对象的大小，并将它们分布到多个列族中，这个过程被称为分片，使用sharding时，将访问修改频率相近的密钥分组，以提高性能和节省磁盘空间。Sharding可以缓解RocksDB压实的影响，压缩数据库之前，RocksDB需要达到一定的已用空间，这会影响OSD性能，这些操作与已用空间级别无关，可以更精确地进行压缩，并将对OSD性能的影响降到最低

Red Hat建议配置的RocksDB空间至少为数据设备大小的4%

在Red Hat Ceph Storage 5中，默认启用分片，从早期版本迁移过来的集群的osd中没有启用分片，从以前版本迁移过来的集群中的osd将不会启用分片

使用ceph config  get验证一个OSD是否启用了sharding，并查看当前的定义

```bash
[ceph: root@clienta /]# ceph config get osd.1 bluestore_rocksdb_cf
true
[ceph: root@clienta /]# ceph config get osd.1 bluestore_rocksdb_cfs
m(3) p(3,0-12) O(3,0-13)=block_cache={type=binned_lru} L P
```

在大多数Ceph用例中，默认值会带来良好的性能。生产集群的最佳分片定义取决于几个因素，Red Hat建议使用默认值，除非面临显著的性能问题。在生产升级的集群中，可能需要权衡在大型环境中为RocksDB启用分片所带来的性能优势和维护工作

可以使用BlueStore管理工具ceph-bluestore-tool重新共享RocksDB数据库，而无需重新配置osd。要重新共享一个OSD，需要停止守护进程并使用--sharding选项传递新的sharding定义。--path选项表示OSD数据Location，默认为`/var/lib/ceph/$fsid/osd.$ID/`

```bash
[ceph: root@node /]# ceph-bluestore-tool --path <data path> --sharding="m{3) p{3,0-12) 0(3,0-13)= block_cache={type=binned_ lru} L P" reshard
```

## 提供 BlueStore OSD

作为存储管理员，可以使用Ceph Orchestrator服务在集群中添加或删除osd，添加OSD时，需要满足以下条件:

1. 设备不能有分区

2. 设备不能被挂载

3. 设备空间要求5GB以上

4. 设备不能包含Ceph BlueStore OSD

使用ceph orch device ls命令列出集群中主机中的设备

```bash
[ceph: root@clienta /]# ceph orch device ls
Hostname                 Path      Type  Serial  Size   Health   Ident  Fault  Available  
clienta.lab.example.com  /dev/vdb  hdd           10.7G  Unknown  N/A    N/A    Yes        
clienta.lab.example.com  /dev/vdc  hdd           10.7G  Unknown  N/A    N/A    Yes        
clienta.lab.example.com  /dev/vdd  hdd           10.7G  Unknown  N/A    N/A    Yes        
```

Available列中标签为Yes的节点为OSD发放的候选节点。如果需要查看已使用的存储设备，请使用ceph device ls命令

使用ceph orch device zap命令准备设备，该命令删除所有分区并清除设备中的数据，以便将其用于资源配置，使用--force选项确保删除上一个OSD可能创建的任何分区

```bash
[ceph: root@node /]# ceph orch device zap node /dev/vda --force
```

### 回顾BlueStore配置方法

在rhcs5中，cephadm是提供和管理osd的推荐工具，它在后台使用cephvolume工具来进行OSD操作，cephadm工具可能看不到手动操作使用ceph -volume，建议仅为故障排除手动执行ceph-volume OSD

有多种方式提供OSDs与cephadm，根据需要的集群行为考虑适当的方法

### 基于Orchestrator提供

Orchestrator服务可以发现集群主机之间的可用设备，添加设备，并创建OSD守护进程。Orchestrator处理在主机之间平衡的新osd的放置，以及处理BlueStore设备选择。

使用ceph orch apply osd --all-available-devices命令提供所有可用的、未使用的设备

```bash
[ceph: root@node /]# ceph orch apply osd --all-available-devices
```

该命令创建一个OSD服务，名为osd.all-available-devices，使Orchestrator服务能够管理所有OSD供应。Orchestrator从集群中的新磁盘设备和使用ceph orch设备zap命令准备的现有设备自动创建osd

若要禁用Orchestrator自动供应osd，请将非托管标志设置为true

```bash
[ceph: root@node /]# ceph orch apply osd --all-available-devices --unmanaged=true
```

### 基于指定目标提供

可以使用特定的设备和主机创建OSD进程，使用ceph orch daemon add命令创建带有指定主机和存储设备的单个OSD守护进程

```bash
[ceph: root@node /)# ceph orch daemon add osd node:/dev/vdb
```

停止OSD进程，使用带OSD ID的ceph orch daemon stop命令

```bash
[ceph: root@node /]# ceph arch daemon stop osd.12 
```

使用ceph orch daemon rm命令移除OSD守护进程

```bash
[ceph: root@node /)# ceph orch daemon rm osd.12
```

释放一个OSD ID，使用ceph osd rm命令

```bash
[ceph: root@node /]# ceph osd rm 12
```

### 基于服务规范文件提供

使用服务规范文件描述OSD服务的集群布局，可以使用过滤器自定义服务发放，通过过滤器，可以在不知道具体硬件架构的情况下配置OSD服务，这种方法在自动化集群引导和维护窗口时很有用

下面是一个示例服务规范YAML文件，它定义了两个OSD服务，每个服务使用不同的过滤器来放置和BlueStore设备位置

```yaml
service_type: osd
service_ id: osd_size_and_model
placement:
  host_pattern: '*'
data_devices:
  size: '100G:'
db_devices: 
  model: My-Disk 
wal_devices:
  size: '10G:20G' 
unmanaged: true
---
service_type: osd 
service_id: osd_host_and_path 
placement: 
  host_pattern: 'node[6-10]' 
data_devices: 
  paths: 
    - /dev/sdb 
db_devices: 
  paths: 
    - /dev/sdc 
wal_devices: 
  paths: 
    - /dev/sdd 
encrypted: true
```

osd_size_and_model服务指定任何主机都可以用于放置，并且该服务将由存储管理员管理，数据设备必须有一个100gb或更多的设备，提前写日志必须有一个10 - 20gb的设备。数据库设备必须是My-Disk型号

osd_host_and_path服务指定目标主机必须在node6和node10之间的节点上提供，并且服务将由协调器服务管理，数据、数据库和预写日志的设备路径必须 /dev/sdb、 /dev/sdc 和 /dev/sdd，此服务中的设备将被加密

执行ceph orch apply命令应用服务规范

```bash
[ceph: root@node /]# ceph orch apply -i service_spec.yaml 
```

### 其他OSD实用工具

ceph-volume命令是将逻辑卷部署为osd的模块化工具，它在框架类型中使用了插件，ceph -volume实用程序支持lvm插件和原始物理磁盘，它还可以管理由遗留的ceph-disk实用程序提供的设备

使用ceph-volume lvm命令手动创建和删除BlueStore osd，在块存储设备/dev/vdc上创建一个新的BlueStore OSD:

```bash
[ceph: root@node /]# ceph-volume lvm create --bluestore --data /dev/vdc 
```

create子命令的另一种选择是使用ceph-volume lvm prepare和ceph -volume lvm activate子命令，通过这种方法，osd逐渐引入到集群中，可以控制新的osd何时处于up或in状态，因此可以确保大量数据不会意外地在osd之间重新平衡

prepare子命令用于配置OSD使用的逻辑卷，可以指定逻辑卷或设备名称，如果指定了设备名，则会自动创建一个逻辑卷

```bash
[ceph: root@node /]# ceph-volume lvm prepare --bluestore --data /dev/vdc 
```

activate子命令为OSD启用一个systemd单元，使其在启动时启动，使用activate子命令时，需要从命令ceph-vo lume lvm list的输出信息中获取OSD的fsid (UUID)。提供唯一标识符可以确保激活正确的OSD，因为OSD id可以重用

```bash
[ceph: root@node /]# ceph-volume lvm activate <osd-fsid>
```

创建OSD后，使用systemctl start ceph-osd@$id命令启动OSD，使其在集群中处于up状态

batch子命令可以同时创建多个osd。

```bash
[ceph: root@node /]# ceph-volume lvm batch --bluestore /dev/vdc /dev/vdd /dev/nvme0n1 
```

inventory子命令用于查询节点上所有物理存储设备的信息

```bash
[ceph: root@node /]# ceph-volume inventory
```

# 创建和配置池

## 了解池的含义

1. 池是存储对象的逻辑分区。Ceph客户端将对象写入池

2. Ceph客户机需要集群名称(默认情况下是Ceph)和一个监视器地址来连接到集群，Ceph客户端通常从Ceph配置文件中获取这些信息，或者通过指定为命令行参数来获取

3. Ceph客户端使用集群映射检索到的池列表来确定存储新对象的位置

4. Ceph客户端创建一个输入/输出上下文到一个特定的池，Ceph集群使用CRUSH算法将这些池映射到放置组，然后放置组映射到特定的osd

5. 池为集群提供了一层弹性，因为池定义了可以在不丢失数据的情况下发生故障的osd的数量

## 池类型

可用的池类型有复制池和纠删代码池，工作负载的用例和类型可以帮助您确定要创建复制池还是纠删代码池

复制池是默认的池类型，通过将各个对象复制到多个 OSD 来发挥作用，它们需要更多的存储空间， 因为会创建多个对象副本，但读取操作不受副本丢失的影响

纠删代码池需要的存储空间和网络带宽较小，但因为奇偶校验计算，计算开销会更高一些

对于不需要频繁访问且不需要低延迟的数据，纠删代码池通常是更好的选择。对于经常访问并且需要快速读取性能的数据，复制池通常都是更好的选择。每一种池的恢复时间取决于特定的部署和故障情景。

创建池后，不能修改池的类型

## 池属性

在创建池时，您必须指定特定的属性： 

1. pool name，必须在集群中唯一

2. pool type，决定了池用于确保数据持久性的保护机制

3. replicated 类型，将每个对象的多个 副本分发到集群中

4. erasure coded 类型，将每个对象分割为多个区块，并将它们与额外的纠删代码区块一起分发，以使用自动纠错机制来保护对象

5. 池中的 placement groups (PG) 数量，这将其对象存储到由 CRUSH 算法决定的一组 OSD 中

6. 可选的 CRUSH rule set，Ceph 使用它来标识要用于存储池对象的放置组

更改 osd_pool_default_pg_num 和 osd_pool_default_pgp_num 配置设置，以设置池的默认 PG 数

## 创建复制池

Ceph通过为每个对象创建多个副本来保护复制池中的数据，Ceph使用CRUSH故障域来确定作用集的主要osd来存储数据，然后，Ceph使用CRUSH 故障域来决定主OSD来存储数据，然后住OSD查找当前的池副本数量并计算辅助OSD来写入数据，当主OSD收到写响应并完成写操作后，主OSD确认写成功到Ceph客户端，这样可以在一个或多个osd失效时保护对象中的数据

使用以下命令创建一个复制池

```bash
[ceph: root@node /]# ceph osd pool create pool-name pg-num pgp-num replicated crush-rule-name 
```

其中：

1. pool_name 是新池的名称

2. pg_num 是为这个池配置的放置组 (PG) 总数

3. pgp_num 是这个池的有效放置组数量，将它设置为与 pg_num 相等

4. replicated 指定这是复制池，如果命令中未包含此参数，这是默认值

5. crush-rule-name 是想要⽤于这个池的 CRUSH 规则集的名称，osd_pool_default_crush_replicated_ruleset 配置参数设置其默认值

在初始配置池之后，可以调整池中放置组的数量，如果pg_num和pgp_num被设置为相同的数字，那么以后任何对pg_num的调整都会自动进行调整pgp_num的值。如果需要，对pgp_num的调整会触发跨osd的pg移动，以实现更改，使用以下命令在池中定义新的pg数量

```bash
[ceph: root@node /]# ceph osd pool set my_pool pg_num 32
```

使用ceph osd pool create命令创建池时，不指定副本个数(size)，osd_pool _default size配置参数定义了副本的数量，默认值为3

```bash
[ceph: root@node /]# ceph config get mon osd_pool_default_size 
3 
```

使用ceph osd pool set pooI-name size number-of-replica 命令修改池大小，或者，更新osd_pool_default_size配置设置的默认设置

osd_pool_default_min_size参数设置一个对象的拷贝数，必须可以接受I/O的请求，缺省值为2

## 配置Erasure编码池

Erasure编码池使用擦除编码代替复制来保护对象数据

存储在Erasure编码池中的对象被划分为多个数据块，这些数据块存储在单独的osd中，编码块的数量是根据数据块计算出来的，并存储在不同的osd中，当OSD出现故障时，编码块用于重建对象的数据，主OSD接收到写操作后，将写载荷编码成K+M块，通过Erasure编码池发送给备OSD

Erasure编码池使用这种方法来保护它们的对象，并且与复制池不同，它不依赖于存储每个对象的多个副本

总结Erasure编码池的工作原理:

1. 每个对象的数据被划分为k个数据块

2. 计算M个编码块

3. 编码块大小与数据块大小相同

4. 该对象总共存储在k + m个osd上

![](https://gitee.com/cnlxh/ceph/raw/master/images/component/configuring-config-erasure.svg)

Erasure编码比复制更有效地利用存储容量，复制池维护一个对象的n个副本，而纠删编码只维护k + m块，例如，3副本的复制池使用3倍的存储空间，k=4和m=2的Erasure编码池只使用1.5倍的存储空间

Red Hat支持以下k+m值，从而产生相应的可用到原始比率:

4 + 2(1:1.5比率)

8 + 3(1:1.375比率)

8 + 4(1:1.5比率)

erasure code开销的计算公式为n0SD * k / (k+m) * 0SD大小，例如，如果您有64个4TB的osd(总共256 TB)， k=8, m=4，那么公式是64 * 8 /(8+4)* 4 = 170.67，然后将原始存储容量除以开销，得到这个比率。256TB /170.67 TB = 1.5

与复制池相比，Erasure编码池需要更少的存储空间才能获得类似级别的数据保护，从而降低存储集群的成本和规模。但是，计算编码块会增加Erasure编码池的CPU处理和内存开销，从而降低整体性能

使用以下命令创建Erasure编码池

```bash
[ceph: root@node /]# ceph osd pool create pool-name pg-num pgp-num erasure erasure-code-profile crush-rule-name 
```

其中：

1. pool-name 是新池的名称

2. pg-num 是这个池的放置组 (PG) 总数

3. pgp-num 是这个池的有效放置组数量，通常而言，这应当与 PG 总数相等

4. erasure 指定这是纠删代码池

5. erasure-code-profile 要使⽤的配置文件的名称，可以使用ceph osd erasure-code-profile set 命令创建新的配置⽂件，配置文件定义 k 和 m 值，以及要使用的纠删代码池插件。默认情况下，Ceph 使用default 配置文件

6. crush-rule-name 是要用于这个池的 CRUSH 规则集的名称。如果不设置，Ceph 将使用纠删代码池配置文件中定义的规则集

可以在池上配置放置组自动伸缩，自动缩放允许集群计算放置组的数量，并自动选择适当的pg_num值，自动缩放在Red Hat Ceph Storage 5中是默认启用的

集群中的每个池都有一个pg_autoscale_mode选项，其值为on、off或warn

1. on:启用自动调整池的PG计数

2. off:禁用池的PG自动伸缩

3. warn:当PG计数需要调整时，引发健康警报并将集群健康状态更改为HEALTH_WARN

本例在Ceph MGR节点上启用pg_autoscaler模块，并将池的自动缩放模式设置为on:

```bash
[ceph: root@node /]# ceph mgr module enable pg_autoscaler 
module 'pg_autoscaler' is already enabled (always-on) 
[ceph: root@node /]# ceph osd pool set pool-name pg_autoscale_mode on 
set pool 7 pg_autoscale_mode to on
```

纠删代码池不能使用对象映射特性，对象映射是一个对象索引，跟踪rbd对象块的分配位置，拥有池的对象映射可以提高调整大小、导出、扁平化和其他操作的性能。

## Erasure Code配置文件

Erasure Code配置文件配置你的Erasure Code池用来存储对象的数据块和编码块的数量，以及使用哪些Erasure Code插件和算法

创建配置文件来定义不同的纠删编码参数集，Ceph在安装过程中自动创建默认概要文件，这个配置文件被配置为将对象分为两个数据块和一个编码块

使用以下命令创建一个新的概要文件

```bash
[ceph: root@node /]# ceph osd erasure-code-profile set profile-name arguments
```

以下是可用的参数:

**k**

跨osd分割的数据块的数量，缺省值为2

**m**

数据不可用前可能发生故障的osd数量，缺省值为1

**directory**

这个可选参数是插件库的位置，默认值为/usr/lib64/ceph/erasure-code

**plugin**

此可选参数定义要使用的纠删编码算法

**crush-failure-domain**

这个可选参数定义了CRUSH故障域，它控制块的放置，默认情况下，它被设置为host，这确保一个对象的块被放置在不同主机的osd上，如果设置为osd，那么一个对象的chunk可以放置在同一主机上的osd上，将故障域设置为osd，会导致主机上所有的osd故障，弹性较差，主机失败，可以定义并使用故障域，以确保块放置在不同数据中心机架或其他指定的主机上的osd上

**crush -device-class**

此可选参数仅为池选择由该类设备支持的osd，典型的类可能包括hdd、ssd或nvme

**crush-root**

该可选参数设置CRUSH规则集的根节点

**key=value**

插件可能具有该插件特有的键值参数

**technique**

每个插件提供一组不同的技术，用于实现不同的算法

不能修改已存在存储池的erasure code配置文件



使用ceph osd erasure-code-profile ls 命令列出已存在的配置文件

使用ceph osd erasure-code-profile get命令查看已创建配置文件的详细信息

使用ceph osd erasure-code-profile rm删除已存在的配置文件

## 管理和操作池

可以查看、修改已创建的存储池，并修改存储池的配置信息

1. 使用ceph osd pool rename命令重命名池，这不会影响存储在池中的数据，如果重命名池，并且为经过身份验证的用户提供了每个池的功能，则必须使用新的池名称更新用户的功能

2. 使用ceph osd pool Delete命令删除osd池

3. 使用ceph osd pool set pool_name nodelete true命令可以防止指定池被删除，使用实例将nodedelete设置为false以允许删除池

4. 使用ceph osd pool set和ceph osd pool get命令查看和修改池配置

5. 使用ceph osd lspool ls和ceph osd pool ls detail命令列出池和池配置设置

6. 使用ceph df和ceph osd pool stats命令列出池的使用情况和性能统计信息

7. 使用ceph osd pool application enable命令启用池中的Ceph应用，应用类型为Ceph File System的cepfs、Ceph Block Device的rbd、RADOS Gateway的rgw

8. 使用ceph osd pool set-quota命令设置池配额，限制池中最大字节数或最大对象数

当存储池达到设置的配额时，将阻止操作，可以通过将配额值设置为0来删除配额

配置这些设置值的示例，以启用对池重新配置的保护:

**osd_ pool _default flag_nodelete**

设置池上的nodedelete标志的默认值，设置该值为true，以防止删除池

**osd_pool_default_flag_nopgchange**

设置池上的nopgchange标志的默认值，设置为true可以防止pg_ num和pgp_num的变化

**osd_pool_default_flag_nosizechange**

设置池的nosizechange标志的默认值。设置该值为true，以防止池的大小变化

## 池名称空间

命名空间是池中对象的逻辑组，可以限制对池的访问，以便用户只能在特定的名称空间中存储或检索对象，名称空间的一个优点是限制用户访问池的一部分，名称空间对于限制应用程序的存储访问非常有用，它们允许对池进行逻辑分区，并将应用程序限制到池中的特定名称空间。

可以为每个应用程序专用一个完整的池，但是拥有更多的池意味着每个OSD拥有更多的pg，而pg在计算上是非常昂贵的，随着负载的增加，这可能会降低OSD的性能，使用名称空间，可以保持池的数量相同，而不必为每个应用程序专用整个池

要在名称空间中存储对象，客户机应用程序必须提供池和名称空间名称。默认情况下，每个池包含一个名称为空的名称空间，称为默认名称空间。

使用rados命令从池中存储和检索对象。使用-n name和--namespace=name选项指定要使用的池和命名空间

下面以将/etc/services文件作为srv对象存储在系统命名空间下的mytestpool池中为例

```bash
[ceph: root@node /]# rados -p mytestpool -N system put srv /etc/services 
[ceph: root@node /]# rados -p mytestpool -N system ls 
srv 
```

使用--all选项列出池中所有名称空间中的所有对象，要获得JSON格式的输出，请添加--format=j son-pretty选项

下面的例子列出了mytestpool池中的对象。mytest对象有一个空的名称空间。其他对象属于system或flowers名称空间

```json
[ceph: root@node /]# rados -p mytestpool --all ls 
system srv 
flowers anemone 
flowers iris 
system magic 
flowers rose 
mytest 
system networks 
[ceph: root@node /]# rados -p mytestpool --all ls --format=json-pretty 
...
```
