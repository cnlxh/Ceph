# 描述Ceph存储用户角色

Ceph 存储管理员一般执行下列任务：

1. 安装、配置和维护 Ceph 存储集群

2. 向基础架构师培训 Ceph 的功能和特性

3. 向用户介绍 Ceph 数据表示和方法，作为可选的数据应用选项

4. 提供弹性和恢复，例如复制、备份和灾难恢复方法

5. 通过基础架构即代码实现自动化和集成

6. 提供使用数据分析和高级大规模数据挖掘的权限

# 描述红帽 Ceph 存储架构

红帽 Ceph 存储提供企业级软件定义存储解决方案，它使用标准硬件和存储设备的服务器。Ceph 采用模块化分布式架构，包含下列元素：

1. 对象存储后端，称为 RADOS（可靠的自主分布式对象存储）

2. 与 RADOS 交互的多种访问方式

RADOS 是一种自我修复、自我管理的软件型对象存储

## Ceph 存储后端组件

RADOS 是 Ceph 存储后端，包含以下守护进程：

1. 监控器 (MON) 维护集群状态映射。它们帮助其他守护进程互相协调

2. 对象存储设备 (OSD) 存储数据并处理数据复制、恢复和重新平衡

3. 管理器 (MGR) 通过基于浏览器的仪表板和 REST API，跟踪运行时指标并公开集群信息。

4. 元数据服务器 (MDS) 存储供 CephFS 使用的元数据（而非对象存储或块存储），让客户端能够高效执行 POSIX 命令。

这些守护进程可以扩展，以满足部署的存储集群的要求

### Ceph 监控器

Ceph 监控器 (MON) 是维护集群映射主要副本的守护进程，集群映射是由五种映射组成的集合，分别是：

1. 监视器映射

2. 管理器映射

3. OSD 映射

4. MDS 映射

5. CRUSH 映射

Ceph 必须处理每个集群事件，更新合适的映射，并将更新后的映射复制到每个监控器守护进程，若要应用更新，MON 必须就集群状态建立共识。这要求配置的监控器中有多数可用且就映射更新达成共识，为 Ceph 集群配置奇数个监控器，以确保监控器能在就集群状态投票时建立仲裁，配置的监控器中必须有超过半数正常发挥作用，Ceph 存储集群才能运行并可访问，这是保护集群数据的完整性所必需的

### Ceph 对象存储设备

Ceph 对象存储设备 (OSD) 是 Ceph 存储集群的构建块，OSD 将存储设备（如硬盘或其他块设备）连接到 Ceph 存储集群。一台存储服务器可以运行多个 OSD 守护进程，并为集群提供多个 OSD，Ceph 旧版本要求OSD 存储设备具有底层文件系统，但 BlueStore 以原始模式使用本地存储设备，这有助于提升性能

Ceph 客户端和 OSD 守护进程都使用可扩展哈希下的受控复制 (CRUSH) 算法来高效地计算对象位置的信息，而不依赖中央服务器查找

CRUSH 将每个对象分配给单个哈希存储桶，称为放置组 (PG)。PG 是对象（应用层）和 OSD（物理层）之间的抽象层，CRUSH 使用伪随机放置算法在 PG 之间分布对象，并且使用规则来确定 PG 到OSD 的映射。出现故障时，Ceph 将 PG 重新映射到不同的物理设备 (OSD) ，并同步其内容以匹配配置的数据保护规则，一个 OSD 是对象放置组的主要 OSD，Ceph 客户端在读取或写入数据时始终联系操作集合中的主要 OSD，其他 OSD 为次要 OSD，在确保集群故障时的数据弹性方面发挥重要作用

Primary OSD 的功能：

1. 服务所有 I/O 请求

2. 复制和保护数据

3. 检查数据的一致性

4. 重新平衡数据

5. 恢复数据

次要 OSD 的功能：

1. 行动始终受到Primary OSD 的控制

2. 能够变为Primary OSD

每个 OSD 具有自己的 OSD ⽇志。OSD 日志提供了对 OSD 实施写操作的性能。来自Ceph 客户端的写操作本质上通常是随机的 I/O，由 OSD 守护进程顺序写入到日志中。当涉及的所有OSD日志记录了写请求后，Ceph 将每个写操作确认到客户端，OSD 然后将操作提交到其后备存储。每隔几秒钟，OSD 会停止向日志写入新的请求，以将 OSD日志的内容应用到后备存储，然后，它会修剪日志中的已提交请求，回收日志存储设备上的空间

当 Ceph OSD 或其存储服务器出现故障时，Ceph 会在 OSD 重新启动后重演其日志，重演序列在最后一个已同步的操作后开始，因为 Ceph 已将同步的日志记录提交到 OSD 的存储，OSD日志使用OSD 节点上的原始卷，若有可能，应在单独的SSD等快速设备上配置日志存储

### Ceph 管理器

Ceph 管理器 (MGR) 提供一系列集群统计数据，集群中不可用的管理器不会给客户端 I/O 操作带来负面影响。在这种情况下，尝试查询集群统计数据会失败，可以在不同的故障域中部署至少两个 Ceph 管理器提升可用性

管理器守护进程将集群中收集的所有数据的访问集中到一处，并通过 TCP 端⼝ 7000（默认）向存储管理员提供一个简单的 Web 仪表板。它还可以将状态信息导出到外部 Zabbix 服务器，将性能信息导出到 Prometheus。Ceph 指标是一种基于 collectd 和 grafana 的监控解决方案，可补充默认的仪表板

### 元数据服务器

Ceph 元数据服务器（MDS）管理 Ceph 文件系统（CephFS）元数据。它提供兼容 POSIX 的共享文件系统元数据管理，包括所有权、时间戳和模式。MDS 使用RADOS 而非本地存储来存储其元数据，它无法访问文件内容

MDS 可让 CephFS 与 Ceph 对象存储交互，将索引节点映射到对象，并在树内映射 Ceph 存储数据的位置，访问 CephFS 文件系统的客户端首先向 MDS 发出请求，这会提供必要的信息以便从正确的OSD 获取文件内容

### 集群映射

Ceph 客户端和对象存储守护进程 OSD 需要确认集群拓扑。五个映射表示集群拓扑，统称为集群映射。Ceph 监控器守护进程维护集群映射的主副本。Ceph MON 集群在监控器守护进程出现故障时确保高可用性

**监控器映射** 包含集群 fsid、各个监控器的位置、名称、地址和端口，以及映射时间戳。使用ceph mon dump 来查看监控器映射。fsid 是一种自动生成的唯⼀标识符 (UUID)，用于标识 Ceph 集群

**OSD 映射** 包含集群 fsid、池列表、副本大小、放置组编号、OSD 及其状态的列表，以及映射时间戳。使用ceph osd dump查看OSD 映射 

**放置组 (PG) 映射** 包含 PG 版本、全满比率、每个放置组的详细信息，例如 PG ID、就绪集合、操作集合、PG 状态、每个池的数据使用量统计、以及映射时间戳。使用ceph pg dump查看包含的PG 映射统计数据

**CRUSH 映射** 包含存储设备的列表、故障域层次结构（例如设备、主机、机架、行、机房），以及存储数据时层次结构的规则，若要查看CRUSH 映射，首先使用ceph osd getcrushmap -o comp-filename，使用crushtool -d comp-filename -o decomp-filename 解译该输出，使用文本编辑器查看解译后的映射

**元数据服务器 (MDS) 映射** 包含用于存储元数据的池、元数据服务器列表、元数据服务器状态和映射时间戳。查看包含 ceph fs dump 的 MDS映射

## Ceph 访问方式

Ceph 提供四种访问 Ceph 集群的方法：

1. Ceph 原生API (librados)

2. Ceph 块设备（RBD、librbd），也称为 RADOS 块设备 (RBD) 镜像

3. Ceph 对象网关

4. Ceph 文件系统（CephFS、libcephfs）

下图描述了Ceph集群的四种数据访问方法，支持访问方法的库，以及管理和存储数据的底层Ceph组件

![ceph-components](https://gitee.com/cnlxh/ceph/raw/master/images/intro/ceph-components.svg)

### Ceph 原生API (librados)

librados 是原生C 库，允许应用直接使用RADOS 来访问 Ceph 集群中存储的对象，有可以用C++、Java、Python、Ruby、Erlang 和 PHP，编写软件以直接与 librados 配合使用可以提升性能，为了简化对 Ceph 存储的访问，也可以改为使用提供的更高级访问方式，如 RADOS 块设备、Ceph 对象网关 (RADOSGW) 和 CephFS

### RADOS 块设备

Ceph 块设备（RADOS 块设备或 RBD）通过 RBD 镜像在 Ceph 集群内提供块存储。Ceph 分散在集群不同的 OSD 中构成 RBD 镜像的个体对象。由于组成 RBD 的对象分布到不同的 OSD，对块设备的访问自动并行处理

RBD 提供下列功能：

1. Ceph 集群中虚拟磁盘的存储

2. Linux 内核中的挂载支持

3. QEMU、KVM 和 OpenStack Cinder 的启动支持

### Ceph 对象网关（RADOS 网关）

Ceph 对象网关（RADOS 网关、RADOSGW 或 RGW）是使用librados 构建的对象存储接口。它使用这个库来与 Ceph 集群通信并且直接写入到 OSD 进程。它通过 RESTful API 为应⽤提供了网关，并且支持两种接口：Amazon S3 和 OpenStack Swift

Ceph 对象网关提供扩展支持，它不限制可部署的网关数量，而且支持标准的 HTTP 负载均衡器。它解决的这些案例包括：

1. 镜像存储（例如，SmugMug 和 Tumblr）

2. 备份服务

3. 文件存储和共享（例如，Dropbox）

### Ceph 文件系统 (CephFS)

Ceph 文件系统 (CephFS) 是一种并行文件系统，提供可扩展的、单层级结构共享磁盘，Ceph 元数据服务器 (MDS) 管理与 CephFS 中存储的文件关联的元数据 ，这包括文件的访问、更改和修改时间戳等信息

## Ceph客户端组件

支持云计算的应用程序需要一个有异步通讯能力的简单对象存储接口，Ceph存储集群提供了这样的接口。客户端直接并行访问对象，包括:

1. 池操作

2. 快照

3. 读/写对象
   
   1. 创建或删除
   
   2. 整个对象或字节范围
   
   3. 追加或截断

4. 创建/设置/获取/删除XATTRs

5. 创建/设置/获取/删除键/值对

6. 复合操作和dual-ack语义

当客户端写入RBD映像时，对象映射跟踪后端已存在的RADOS对象，当写入发生时，它会被转换为后端RADOS对象中的偏移量，当对象映射特性启用时，将跟踪RADOS对象的存在以表示对象存在，对象映射保存在librbd客户机的内存中，以避免在osd中查询不存在的对象

对象映射对于某些操作是有益的，例如:

1. 重新调整大小

2. 导出

3. 复制

4. 平

5. 删除

6. 读

存储设备有吞吐量限制，这会影响性能和可伸缩性。存储系统通常支持条带化，即跨多个存储设备存储连续的信息片段，以提高吞吐量和性能。当向集群写入数据时，Ceph客户端可以使用数据分条来提高性能

## Ceph 中的数据分布和整理

### 使用池对存储进行分区

Ceph OSD 保护并持续检查集群中存储的数据的完整性，Pools 是 Ceph 存储集群的逻辑分区，用于将对象存储在共同的名称标签下。Ceph 给每个池分配特定数量的哈希存储桶，名为放置组 (PG)，将对象分组到一起进行存储。每个池具有下列可调整属性：

1. 不变 ID

2. 名称

3. 在 OSD 之间分布对象的 PG 数量

4. CRUSH 规则，用于确定这个池的 PG 映射

5. 保护类型（复制或纠删代码）

6. 与保护类型相关的参数

7. 影响集群行为的各种标志

分配给每个池的放置组数量可以独立配置，以匹配数据的类型以及池所需要的访问权限
CRUSH 算法用于确定托管池数据的OSD，每个池分配一条 CRUSH 规则作为其放置策略，CRUSH规则决定哪些 OSD 存储分配了该规则的所有池的数据

### 放置组

放置组 (PG) 将一系列对象聚合到一个哈希存储桶或组中。Ceph 将每个 PG 映射到一组 OSD。一个对象属于一个 PG，但属于同一PG 的所有对象返回相同的散列结果

CRUSH 算法根据对象名称的散列，将对象映射至其 PG。这种放置策略也被称为 CRUSH 放置规则，放置规则标识在 CRUSH 拓扑中选定的故障域，以接收各个副本或纠删码区块

当客户端将对象写入到池时，它使用池的 CRUSH 放置规则来确定对象的放置组。客户端然后使用其集群映射的副本、放置组以及 CRUSH 放置规则来计算对象的副本（或其纠删码区块）应写入到哪些 OSD 中

当新的 OSD 可供 Ceph 集群使用时，放置组提供的间接层非常重要。在集群中添加或移除 OSD 时，放置组会自动在正常运作的 OSD 之间重新平衡

### 将对象映射到其关联的 OSD

1. Ceph 客户端从监控器获取集群映射的最新副本。集群映射向客户端提供有关集群中所有MON、OSD 和 MDS 的信息。它不会向客户端提供对象的位置，客户端必须使用CRUSH 来计算它需要访问的对象位置

2. 要为对象计算放置组 ID，Ceph 客户端需要对象 ID 以及对象的存储池名称。客户端计算 PG ID，这是对象 ID 模数哈希的 PG 数量。接着，它根据池名称查找池的数字 ID，再将池 ID 作为前缀添加到PG ID 中

3. 然后，使用CRUSH 算法确定哪些 OSD 负责某一个 PG（操作集合）。操作集合中目前就绪的 OSD位于就绪集合中，就绪集合中的第一个 OSD 是对象放置组的当前主要 OSD，就绪集合中的所有其他OSD 为次要 OSD

4. Ceph 客户端然后可以直接与主要 OSD 交互，以访问对象

### 数据保护

和 Ceph 客⼾端一样，OSD 守护进程使用CRUSH 算法，但 OSD 守护进程使用它来计算对象副本的存储位置以及用于重新平衡存储。在典型的写入场景中，Ceph 客户端使用CRUSH 算法计算原始对象的存储位置，将对象映射到池和放置组，然后使用CRUSH 映射来确定映射的放置组的主要OSD。在创建池时，将它们设置为复制或纠删代码池

<img title="" src="https://gitee.com/cnlxh/ceph/raw/master/images/intro/ceph-pool-protection.svg" alt="" width="914">

为了提高弹性，为池配置在出现故障时不会丢失数据的 OSD 数量。对于复制池（默认的池类型），该数量决定了在不同设备之间创建和分布对象的副本数。复制池以较低的可用存储与原始存储比为代价，在所有用例中提供更佳的性能

纠删代码提供了更经济高效的数据存储方式，但性能更低。对于纠删代码池，配置值确定要创建的编码块和奇偶校验块的数量

纠删代码的主要优势是能够提供极高的弹性和持久性。还可以配置要使用的编码区块（奇偶校验）数量，RADOS 网关和 RBD 访问方法都支持纠删代码

下图演示了如何在Ceph集群中存储数据对象。Ceph 将池中的一个或多个对象映射到一个 PG，由彩色框表示。此图上的每一个 PG 都被复制并存储在 Ceph 集群的独立 OSD 上

<img title="" src="https://gitee.com/cnlxh/ceph/raw/master/images/intro/crush-placement-groups.svg" alt="https://raw.githubusercontent.com/cnlxh/Ceph/fd32fdcd5f11595a4a421b44121d3c50acf71fb0/images/intro/ceph-pool-protection.svg" width="689" data-align="inline">



# 描述Ceph存储管理接口

## 介绍Ceph接口

以前的Ceph版本使用Ceph-ansible软件中的Ansible Playbooks进行部署并管理集群，Red Hat Ceph Storage 5引入了cephadm作为工具来管理集群的整个生命周期(部署、管理和监控)，替换之前的ceph-ansible提供的功能

Cephadm被视为Manager守护进程(MGR)中的一个模块，这是部署新集群时的第一个守护进程，Ceph集群核心集成了所有的管理任务

Cephadm由Cephadm包装提供，应该在第一个集群节点上安装这个包，它充当引导节点。Ceph 5被部署在容器中，建立并运行Ceph集群的仅有几个安装包要求是cephadm、podman、python3、chrony，容器化版本降低了部署过程中的复杂性和包依赖关系

下图说明了Cephadm如何与其他服务交互

![cephadm](https://gitee.com/cnlxh/ceph/raw/master/images/intro/cephadm.svg)

Cephadm可以登录到容器注册中心来提取Ceph映像，并使用该映像在节点上部署服务。当引导集群时，这个Ceph容器映像是必需的，因为部署的Ceph容器是基于该映像，为了与Ceph集群节点交互，Cephadm使用SSH连接向集群添加新主机、添加存储或监控这些主机

## 探索Ceph管理接口

Ceph部署在容器中，在引导节点中不需要额外的软件，可以从集群的引导节点中的命令行引导集群，引导集群设置了一个最小的集群配置，其中只有一个主机(引导节点)和两个守护进程(监视器和管理进程)，Red Hat Ceph Storage 5提供两个默认部署的接口:Ceph CU和Dashboard GUI

## Ceph编排器

可以使用Ceph编排器轻松地向集群添加主机和守护进程，使用编排器来提供Ceph守护进程和服务，并扩展或收缩集群。通过Ceph orch命令使用Ceph编排器，还可以使用Red Hat Ceph Storage Dashboard接口来运行编排器任务。cephadm脚本与Ceph Manager业务流程模块交互

下面的图表说明了Ceph Orchestrator

![Orchestrator](https://gitee.com/cnlxh/ceph/raw/master/images/intro/cephorch.svg)

#### Ceph命令行接口

Cephadm可以启动一个装有所有必需Ceph包的容器，使用这个容器的命令是cephadm shell，只应该在引导节点中运行此命令，因为在引导集群时，只有这个节点可以访问/etc/ceph中的admin密钥

```bash
[root@clienta ~]# cephadm shell
Inferring fsid 2ae6d05a-229a-11ec-925e-52540000fa0c
Inferring config /var/lib/ceph/2ae6d05a-229a-11ec-925e-52540000fa0c/mon.clienta/config
Using recent ceph image registry.redhat.io/rhceph/rhceph-5-rhel8@sha256:6306de945a6c940439ab584aba9b622f2aa6222947d3d4cde75a4b82649a47ff
[ceph: root@clienta /]# 
```

可以通过破折号直接非交互式执行命令

```bash
[root@clienta ~]# cephadm shell -- ceph -s
Inferring fsid 2ae6d05a-229a-11ec-925e-52540000fa0c
Inferring config /var/lib/ceph/2ae6d05a-229a-11ec-925e-52540000fa0c/mon.clienta/config
Using recent ceph image registry.redhat.io/rhceph/rhceph-5-rhel8@sha256:6306de945a6c940439ab584aba9b622f2aa6222947d3d4cde75a4b82649a47ff
  cluster:
    id:     2ae6d05a-229a-11ec-925e-52540000fa0c
    health: HEALTH_OK
```

### Ceph Dashboard接口

Red Hat Ceph Storage 5 Dashboard GUI通过该接口增强了对许多集群任务的支持，Ceph Dashboard GUI是一个基于web的应用程序，用于监控和管理集群，它以比Ceph CLI更直观的方式提供了集群信息。与Ceph CLI一样，Ceph将Dashboard GUI web服务器作为Ceph -mgr守护进程的一个模块，默认情况下，当创建集群时，Ceph在引导节点中部署Dashboard GUI并使用TCP端口8443

Ceph Dashboard GUI提供了这些特性：

**多用户和角色管理**：可以创建具有多种权限和角色的不同用户帐户

**单点登录**：Dashboard GUI允许通过外部身份提供者进行身份验证

**审计**：可以配置仪表板来记录所有REST API请求

**安全**：Dashboard默认使用SSL/TLS保护所有HTTP连接

Ceph Dashboard GUI还实现了管理和监控集群的不同功能，下面的列表虽然不是详尽的，但总结了重要的管理和监控特点:

#### 管理功能

1. 使用CRUSH map查看集群层次结构

2. 启用、编辑和禁用管理器模块

3. 创建、移除和管理osd

4. 管理iSCSI

5. 管理池

#### 监控功能

1. 检查整体集群健康状况

2. 查看集群中的主机及其服务

3. 查看日志

4. 查看集群警报

5. 检查集群容量

下图显示了Dashboard GUI中的状态屏幕。可以快速查看集群的一些重要参数，如集群状态、集群中的主机数量、osd数量等

![](https://gitee.com/cnlxh/ceph/raw/master/images/intro/gui-dashboard-status.png)


