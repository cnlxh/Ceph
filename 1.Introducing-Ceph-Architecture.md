# 描述Ceph存储用户角色

Ceph 存储管理员一般执行下列任务：

1. 安装、配置和维护 Ceph 存储集群

2. 向基础架构师培训 Ceph 的功能和特性

3. 向用户介绍 Ceph 数据表示和方法，作为可选的数据应用选项

4. 提供弹性和恢复，例如复制、备份和灾难恢复方法

5. 通过基础架构即代码实现自动化和集成

6. 提供使用数据分析和高级大规模数据挖掘的权限

# 描述红帽 Ceph 存储架构

红帽 Ceph 存储提供企业级软件定义存储解决方案，它使用标准硬件和存储设备的服务器。Ceph 采用模块化分布式架构，包含下列元素：

1. 对象存储后端，称为 RADOS（可靠的自主分布式对象存储）

2. 与 RADOS 交互的多种访问方式

RADOS 是一种自我修复、自我管理的软件型对象存储

## Ceph 存储后端组件

RADOS 是 Ceph 存储后端，包含以下守护进程：

1. 监控器 (MON) 维护集群状态映射。它们帮助其他守护进程互相协调

2. 对象存储设备 (OSD) 存储数据并处理数据复制、恢复和重新平衡

3. 管理器 (MGR) 通过基于浏览器的仪表板和 REST API，跟踪运行时指标并公开集群信息。

4. 元数据服务器 (MDS) 存储供 CephFS 使用的元数据（而非对象存储或块存储），让客户端能够高效执行 POSIX 命令。

这些守护进程可以扩展，以满足部署的存储集群的要求

### Ceph 监控器

Ceph 监控器 (MON) 是维护集群映射主要副本的守护进程，集群映射是由五种映射组成的集合，分别是：

1. 监视器映射

2. 管理器映射

3. OSD 映射

4. MDS 映射

5. CRUSH 映射

Ceph 必须处理每个集群事件，更新合适的映射，并将更新后的映射复制到每个监控器守护进程，若要应用更新，MON 必须就集群状态建立共识。这要求配置的监控器中有多数可用且就映射更新达成共识，为 Ceph 集群配置奇数个监控器，以确保监控器能在就集群状态投票时建立仲裁，配置的监控器中必须有超过半数正常发挥作用，Ceph 存储集群才能运行并可访问，这是保护集群数据的完整性所必需的

### Ceph 对象存储设备

Ceph 对象存储设备 (OSD) 是 Ceph 存储集群的构建块，OSD 将存储设备（如硬盘或其他块设备）连接到 Ceph 存储集群。一台存储服务器可以运行多个 OSD 守护进程，并为集群提供多个 OSD，Ceph 旧版本要求OSD 存储设备具有底层文件系统，但 BlueStore 以原始模式使用本地存储设备，这有助于提升性能

Ceph 客户端和 OSD 守护进程都使用可扩展哈希下的受控复制 (CRUSH) 算法来高效地计算对象位置的信息，而不依赖中央服务器查找

CRUSH 将每个对象分配给单个哈希存储桶，称为放置组 (PG)。PG 是对象（应用层）和 OSD（物理层）之间的抽象层，CRUSH 使用伪随机放置算法在 PG 之间分布对象，并且使用规则来确定 PG 到OSD 的映射。出现故障时，Ceph 将 PG 重新映射到不同的物理设备 (OSD) ，并同步其内容以匹配配置的数据保护规则，一个 OSD 是对象放置组的主要 OSD，Ceph 客户端在读取或写入数据时始终联系操作集合中的主要 OSD，其他 OSD 为次要 OSD，在确保集群故障时的数据弹性方面发挥重要作用

Primary OSD 的功能：

1. 服务所有 I/O 请求

2. 复制和保护数据

3. 检查数据的一致性

4. 重新平衡数据

5. 恢复数据

次要 OSD 的功能：

1. 行动始终受到Primary OSD 的控制

2. 能够变为Primary OSD

每个 OSD 具有自己的 OSD ⽇志。OSD 日志提供了对 OSD 实施写操作的性能。来自Ceph 客户端的写操作本质上通常是随机的 I/O，由 OSD 守护进程顺序写入到日志中。当涉及的所有OSD日志记录了写请求后，Ceph 将每个写操作确认到客户端，OSD 然后将操作提交到其后备存储。每隔几秒钟，OSD 会停止向日志写入新的请求，以将 OSD日志的内容应用到后备存储，然后，它会修剪日志中的已提交请求，回收日志存储设备上的空间

当 Ceph OSD 或其存储服务器出现故障时，Ceph 会在 OSD 重新启动后重演其日志，重演序列在最后一个已同步的操作后开始，因为 Ceph 已将同步的日志记录提交到 OSD 的存储，OSD日志使用OSD 节点上的原始卷，若有可能，应在单独的SSD等快速设备上配置日志存储

### Ceph 管理器

Ceph 管理器 (MGR) 提供一系列集群统计数据，集群中不可用的管理器不会给客户端 I/O 操作带来负面影响。在这种情况下，尝试查询集群统计数据会失败，可以在不同的故障域中部署至少两个 Ceph 管理器提升可用性

管理器守护进程将集群中收集的所有数据的访问集中到一处，并通过 TCP 端⼝ 7000（默认）向存储管理员提供一个简单的 Web 仪表板。它还可以将状态信息导出到外部 Zabbix 服务器，将性能信息导出到 Prometheus。Ceph 指标是一种基于 collectd 和 grafana 的监控解决方案，可补充默认的仪表板

### 元数据服务器

Ceph 元数据服务器（MDS）管理 Ceph 文件系统（CephFS）元数据。它提供兼容 POSIX 的共享文件系统元数据管理，包括所有权、时间戳和模式。MDS 使用RADOS 而非本地存储来存储其元数据，它无法访问文件内容

MDS 可让 CephFS 与 Ceph 对象存储交互，将索引节点映射到对象，并在树内映射 Ceph 存储数据的位置，访问 CephFS 文件系统的客户端首先向 MDS 发出请求，这会提供必要的信息以便从正确的OSD 获取文件内容

### 集群映射

Ceph 客户端和对象存储守护进程 OSD 需要确认集群拓扑。五个映射表示集群拓扑，统称为集群映射。Ceph 监控器守护进程维护集群映射的主副本。Ceph MON 集群在监控器守护进程出现故障时确保高可用性

**监控器映射** 包含集群 fsid、各个监控器的位置、名称、地址和端口，以及映射时间戳。使用ceph mon dump 来查看监控器映射。fsid 是一种自动生成的唯⼀标识符 (UUID)，用于标识 Ceph 集群

**OSD 映射** 包含集群 fsid、池列表、副本大小、放置组编号、OSD 及其状态的列表，以及映射时间戳。使用ceph osd dump查看OSD 映射 

**放置组 (PG) 映射** 包含 PG 版本、全满比率、每个放置组的详细信息，例如 PG ID、就绪集合、操作集合、PG 状态、每个池的数据使用量统计、以及映射时间戳。使用ceph pg dump查看包含的PG 映射统计数据

**CRUSH 映射** 包含存储设备的列表、故障域层次结构（例如设备、主机、机架、行、机房），以及存储数据时层次结构的规则，若要查看CRUSH 映射，首先使用ceph osd getcrushmap -o comp-filename，使用crushtool -d comp-filename -o decomp-filename 解译该输出，使用文本编辑器查看解译后的映射

**元数据服务器 (MDS) 映射** 包含用于存储元数据的池、元数据服务器列表、元数据服务器状态和映射时间戳。查看包含 ceph fs dump 的 MDS映射

## Ceph 访问方式

Ceph 提供四种访问 Ceph 集群的方法：

1. Ceph 原生API (librados)

2. Ceph 块设备（RBD、librbd），也称为 RADOS 块设备 (RBD) 镜像

3. Ceph 对象网关

4. Ceph 文件系统（CephFS、libcephfs）

### Ceph 原生API (librados)

librados 是原生C 库，允许应用直接使用RADOS 来访问 Ceph 集群中存储的对象，有可以用C++、Java、Python、Ruby、Erlang 和 PHP，编写软件以直接与 librados 配合使用可以提升性能，为了简化对 Ceph 存储的访问，也可以改为使用提供的更高级访问方式，如 RADOS 块设备、Ceph 对象网关 (RADOSGW) 和 CephFS

### RADOS 块设备

Ceph 块设备（RADOS 块设备或 RBD）通过 RBD 镜像在 Ceph 集群内提供块存储。Ceph 分散在集群不同的 OSD 中构成 RBD 镜像的个体对象。由于组成 RBD 的对象分布到不同的 OSD，对块设备的访问自动并行处理

RBD 提供下列功能：

1. Ceph 集群中虚拟磁盘的存储

2. Linux 内核中的挂载支持

3. QEMU、KVM 和 OpenStack Cinder 的启动支持

### Ceph 对象网关（RADOS 网关）

Ceph 对象网关（RADOS 网关、RADOSGW 或 RGW）是使用librados 构建的对象存储接口。它使用这个库来与 Ceph 集群通信并且直接写入到 OSD 进程。它通过 RESTful API 为应⽤提供了网关，并且支持两种接口：Amazon S3 和 OpenStack Swift

Ceph 对象网关提供扩展支持，它不限制可部署的网关数量，而且支持标准的 HTTP 负载均衡器。它解决的这些案例包括：

1. 镜像存储（例如，SmugMug 和 Tumblr）

2. 备份服务

3. 文件存储和共享（例如，Dropbox）

### Ceph 文件系统 (CephFS)

Ceph 文件系统 (CephFS) 是一种并行文件系统，提供可扩展的、单层级结构共享磁盘，Ceph 元数据服务器 (MDS) 管理与 CephFS 中存储的文件关联的元数据 ，这包括文件的访问、更改和修改时间戳等信息。

下图描述了 Ceph 集群的三种数据访问方法、支持访问方式的库，以及管理和存储数据的底层 Ceph组件：

<img title="" src="https://raw.githubusercontent.com/cnlxh/Ceph/fd32fdcd5f11595a4a421b44121d3c50acf71fb0/images/intro/ceph-components.svg" alt="Ceph 组件" data-align="inline" width="1024">

## Ceph 中的数据分布和整理

### 使用池对存储进行分区

Ceph OSD 保护并持续检查集群中存储的数据的完整性，Pools 是 Ceph 存储集群的逻辑分区，用于将对象存储在共同的名称标签下。Ceph 给每个池分配特定数量的哈希存储桶，名为放置组 (PG)，将对象分组到一起进行存储。每个池具有下列可调整属性：

1. 不变 ID

2. 名称

3. 在 OSD 之间分布对象的 PG 数量

4. CRUSH 规则，用于确定这个池的 PG 映射

5. 保护类型（复制或纠删代码）

6. 与保护类型相关的参数

7. 影响集群行为的各种标志

分配给每个池的放置组数量可以独立配置，以匹配数据的类型以及池所需要的访问权限
CRUSH 算法用于确定托管池数据的OSD，每个池分配一条 CRUSH 规则作为其放置策略，CRUSH规则决定哪些 OSD 存储分配了该规则的所有池的数据

### 放置组

放置组 (PG) 将一系列对象聚合到一个哈希存储桶或组中。Ceph 将每个 PG 映射到一组 OSD。一个对象属于一个 PG，但属于同一PG 的所有对象返回相同的散列结果

CRUSH 算法根据对象名称的散列，将对象映射至其 PG。这种放置策略也被称为 CRUSH 放置规则，放置规则标识在 CRUSH 拓扑中选定的故障域，以接收各个副本或纠删码区块

当客户端将对象写入到池时，它使用池的 CRUSH 放置规则来确定对象的放置组。客户端然后使用其集群映射的副本、放置组以及 CRUSH 放置规则来计算对象的副本（或其纠删码区块）应写入到哪些 OSD 中

当新的 OSD 可供 Ceph 集群使用时，放置组提供的间接层非常重要。在集群中添加或移除 OSD 时，放置组会自动在正常运作的 OSD 之间重新平衡

### 将对象映射到其关联的 OSD

1. Ceph 客户端从监控器获取集群映射的最新副本。集群映射向客户端提供有关集群中所有MON、OSD 和 MDS 的信息。它不会向客户端提供对象的位置，客户端必须使用CRUSH 来计算它需要访问的对象位置

2. 要为对象计算放置组 ID，Ceph 客户端需要对象 ID 以及对象的存储池名称。客户端计算 PG ID，这是对象 ID 模数哈希的 PG 数量。接着，它根据池名称查找池的数字 ID，再将池 ID 作为前缀添加到PG ID 中

3. 然后，使用CRUSH 算法确定哪些 OSD 负责某一个 PG（操作集合）。操作集合中目前就绪的 OSD位于就绪集合中，就绪集合中的第一个 OSD 是对象放置组的当前主要 OSD，就绪集合中的所有其他OSD 为次要 OSD

4. Ceph 客户端然后可以直接与主要 OSD 交互，以访问对象

### 数据保护

和 Ceph 客⼾端一样，OSD 守护进程使用CRUSH 算法，但 OSD 守护进程使用它来计算对象副本的存储位置，以及用于重新平衡存储。在典型的写入场景中，Ceph 客户端使用CRUSH 算法计算原始对象的存储位置，将对象映射到池和放置组，然后使用CRUSH 映射来确定映射的放置组的主要OSD。在创建池时，将它们设置为复制或纠删代码池

<img src="https://raw.githubusercontent.com/cnlxh/Ceph/fd32fdcd5f11595a4a421b44121d3c50acf71fb0/images/intro/ceph-pool-protection.svg" title="" alt="https://raw.githubusercontent.com/cnlxh/Ceph/fd32fdcd5f11595a4a421b44121d3c50acf71fb0/images/intro/ceph-pool-protection.svg" width="914">

为了提高弹性，为池配置在出现故障时不会丢失数据的 OSD 数量。对于复制池（默认的池类型），该数量决定了在不同设备之间创建和分布对象的副本数。复制池以较低的可用存储与原始存储比为代价，在所有用例中提供更佳的性能

纠删代码提供了更经济高效的数据存储方式，但性能更低。对于纠删代码池，该数值决定了要创建的编码区块和奇偶校验块的数量

纠删代码的主要优势是能够提供极高的弹性和持久性。还可以配置要使用的编码区块（奇偶校验）数量。RADOS 网关和 RBD 访问方法都支持纠删代码

下图演示了数据对象如何在 Ceph 集群中存储。Ceph 将池中的一个或多个对象映射到单个 PG，由彩色框表示。此图上的每一个 PG 都复制并存储在 Ceph 集群的单个 OSD 上

<img title="" src="https://raw.githubusercontent.com/cnlxh/Ceph/fd32fdcd5f11595a4a421b44121d3c50acf71fb0/images/intro/crush-placement-groups.svg" alt="https://raw.githubusercontent.com/cnlxh/Ceph/fd32fdcd5f11595a4a421b44121d3c50acf71fb0/images/intro/ceph-pool-protection.svg" width="689">










