# 部署红帽 Ceph 存储

## 准备集群部署

本次采用cephadm工具来进行部署，cephadm会包含两个组件

1. cephadm shell

2. cephadm orchestrator

cephadm shell命令在ceph提供的管理容器中运行一个bash shell，最初使用cephadm shell执行集群部署任务、安装并运行集群后执行集群管理任务。

启动cephadm shell以交互方式运行单个或多个命令，要以交互方式运行它，应使用cephadm shell命令打开shell，然后运行Ceph命令。

cephadm 提供了一个命令行来编排ceph-mgr模块，该模块与外部编排服务进行对接，编排器的作用是协调必须跨多个节点和服务协作进行配置更改

```bash
[root@clienta ~]# cephadm shell
Inferring fsid 2ae6d05a-229a-11ec-925e-52540000fa0c
Inferring config /var/lib/ceph/2ae6d05a-229a-11ec-925e-52540000fa0c/mon.clienta/config
Using recent ceph image registry.redhat.io/rhceph/rhceph-5-rhel8@sha256:6306de945a6c940439ab584aba9b622f2aa6222947d3d4cde75a4b82649a47ff
[ceph: root@clienta /]# 
```

如果要执行非交互式的单个命令，可以用两个破折号连接

```bash
[root@clienta ~]# cephadm shell -- ceph osd pool ls
Inferring fsid 2ae6d05a-229a-11ec-925e-52540000fa0c
Inferring config /var/lib/ceph/2ae6d05a-229a-11ec-925e-52540000fa0c/mon.clienta/config
Using recent ceph image registry.redhat.io/rhceph/rhceph-5-rhel8@sha256:6306de945a6c940439ab584aba9b622f2aa6222947d3d4cde75a4b82649a47ff
device_health_metrics
.rgw.root
default.rgw.log
default.rgw.control
default.rgw.meta
```

### 规划服务托管

所有集群服务现在都作为容器运行。容器化的Ceph服务可以运行在同一个节点上;这叫做“托管”。Ceph服务的托管允许更好地利用资源，同时保持服务之间的安全隔离。支持与OSD配置的守护进程有:RADOSGW、MOS、RBD-mirror、MON、MGR、Grafana、NFS Ganesha

### 节点之间的安全通讯

cephadm命令使用SSH协议与存储集群节点通信。集群SSH Key是在集群引导过程中创建的。将集群公钥复制到每个主机，使用如下命令复制集群密钥到集群节点:

```bash
[root@node -]# cephadm shell 
[ceph: root@node /]# ceph cephadm get-pub-key > ~/ceph.pub 
[ceph: root@node /]# ssh-copy-id -f -i ~/ceph.pub root@node.example.com
```

## 部署新集群

部署新集群的步骤如下：

1. 在选择作为引导节点的主机上安装cephadm-ansible包，它是集群中的第一个节点。

2. 在节点上执行cephadm的预检查playbook。该剧本验证主机是否具有所需的先决条件。

3. 使用cephadm引导集群。引导过程完成以下任务:
   
   1. 在引导节点上安装并启动Ceph Monitor和Ceph Manager守护进程
   
   2. 创建/etc/ceph目录
   
   3. 拷贝一份集群SSH公钥到"/etc/ceph/ceph"，并添加密钥到/root/.ssh/authorized_keys文件中
   
   4. 将与新集群通信所需的最小配置文件写入/etc/ceph/ceph.conf文件
   
   5. 写入client.admin管理密钥到/etc/ceph/ceph.client.admin.keyring
   
   6. 为prometheus和grafana服务以及其他工具部署一个基本的监控stack

### 安装先决条件

在引导节点上安装cephadm-ansible

```bash
[root@node ~]# yum install cephadm-ansible
```

运行cephadm-preflight，这个剧本配置Ceph存储库并为引导准备存储集群。它还安装必要的包，例如Podman、lvm2、chrony和cephaldm

cephadm-preflight使用cephadm-ansible inventory文件来识别admin和client节点

inventory默认位置是/usr/share/cephadm-ansible/hosts，下面的例子展示了一个典型的inventory文件的结构:

```ini
[admin]
node00
[clients]
client01
client02
client03
```

运行cephadm-preflight

```bash
[root@node ~]# ansible-playbook -i INVENTORY-FILE cephadm-preflight.yml --extra-vars "ceph_origin=rhcs"
```

### 引导集群

cephadm引导过程在单个节点上创建一个小型存储集群，包括一个Ceph Monitor和一个Ceph Manager，以及任何必需的依赖项

通过使用ceph orchestrator命令或Dashboard GUI扩展存储集群添加集群节点和服务

#### 使用cephadm bootstrap引导新集群

```bash
[root@node -]# cephadm bootstrap --mon-ip=MON_IP \
--registry-url=registry.redhat.io \
--registry-username=REGISTRY_USERNAME \
--registry-password=REGISTRY_PASSWORD \ 
--initial-dashboard-password=DASHBOARO_PASSWORO \
--dashboard-password-noupdate \ 
--allow-fqdn-hostname
```

运行结束后，会输出以下内容

```bash
Ceph Dashboard is now available at: 
URL: https://boostrapnode.example.com:8443/ 
User: admin 
Password: adminpassword 
You can access the Ceph CLI with: 
sudo /usr/sbin/cephadm shell --fsid 266ee7a8-2a05-lleb-b846-5254002d4916 -c /etc/ceph/ceph.conf -k /etc/ceph/ceph.client.admin.keyring 
Please consider enabling telemetry to help improve Ceph: 
ceph telemetry on
```

#### 使用服务规范文件

cephadm bootstrap命令使用--apply-spec选项和服务规范文件用于引导存储集群并配置其他主机和守护进程，配置文件是一个YAML文件，其中包含服务类型、位置和要部署服务的指定节点

服务配置文件示例如下:

```yaml
service_type: host
addr: node-00
hostname: node-00
---
service_type: host
addr: node-01
hostname: node-01
---
service_type: host
addr: node-02
hostname: node-02
---
service_type: mon
placement:
  hosts:
    - node-00
    - node-01
    - node-02
---
service_type: mgr
placement:
  hosts:
    - node-00
    - node-01
    - node-02
---
service_type: rgw
service_id: realm.zone
placement:
  hosts:
    - node-01
    - node-02
---
service_type: osd
placement:
  host_pattern: "*"
data_devices:
  all:true
```

```bash
[root@node ~]# cephadm bootstrap --apply-spec CONFIGURATION_FILE_NAME --mon-ip MONITOR-IP-ADDRESS 
```

## 为集群节点打标签

Ceph协调器支持为主机分配标签，标签可以用于对集群进行分组hosts，以便可以同时将Ceph服务部署到多个主机，主机可以有多个标签

标签可以帮助识别每个主机上运行的守护进程，从而简化集群管理任务，例如，您可以使用ceph orch host ls或YAML服务规范文件在特定标记的主机上部署或删除守护进程，可以使用ceph orch host ls命令来列出我们可以用于编排器或YAML服务规范文件，在指定的标签节点上用于部署或删除特定的守护进程

除了_admin标签外，标签都是自由形式，没有特定的含义，可以使用标签，如mon, monitor, mycluster_monitor，或其他文本字符串标签和分组集群节点。例如，将mon标签分配给部署mon守护进程的节点，为部署mgr守护进程的节点分配mgr标签，并为RADOS分配rgw网关

例如，下面的命令将_ admin标签应用到主机，以指定为admin节点

```bash
[ceph: root@node /)# ceph orch host label add AOMIN_NOOE _admin
```

使用标签将集群守护进程部署到特定的主机

```bash
(ceph: root@node /)# ceph orch apply prometheus --placement="label:prometheus"
```

## 设置Admin节点

配置admin节点的步骤如下:

1. 将admin标签分配给节点

2. 复制admin密钥到管理节点

3. 复制ceph.conf文件到admin节点

```bash
[root@node ~]# scp /etc/ceph/ceph.client.admin.keyring ADMIN_NODE:/etc/ceph/ 
[root@node ~]# scp /etc/ceph/ceph.conf ADMIN_NODE:/etc/ceph/
```

# 执行Ceph存储集群扩容

有两种方法可以扩展集群中的存储空间:

1. 向集群中添加额外的OSD节点，这称为横向扩展

2. 向以前的OSD节点添加额外的存储空间，这称为纵向扩展

在开始部署额外的osd之前使用cephadm shell -- ceph health命令确保集群处于HEALTH_OK状态

## 配置更多的OSD服务器

作为存储管理员，可以向Ceph存储集群添加更多主机，以维护集群健康并提供足够的负载容量。在当前存储空间已满的情况下，可以通过增加一个或多个osd来增加集群存储容量。

### 分发ssh密钥

作为root用户，将Ceph存储集群SSH公钥添加到新主机上root用户的authorized_keys文件中

```bash
[root@adm ~]# ssh-copy-id -f -i /etc/ceph/ceph.pub root@new-osd-1
```

### 检查并配置先决条件

作为root用户，将新节点添加到目录/usr/share/cephadm-ansible/hosts/的inventory文件中，使用--limit选项运行preflight剧本，以限制剧本的任务只在指定的节点上运行，Ansible Playbook会验证待添加的节点是否满足前置要求

```bash
[root@adm ~]# ansible-playbook -i /usr/share/cephadm-ansible/hosts/ /usr/share/cephadm-ansible/cephadm-preflight.yml --limit new-osd-1
```

### 选择添加主机的方法

#### 使用命令

以root用户，在Cephadm shell下，使用ceph orch host add命令添加一个存储集群的新主机，在本例中，该命令还分配主机标签

```bash
[ceph: root@adm /]# ceph orch host add new-osd-1 --labels=mon,osd,mgr
```

#### 使用规范文件添加多个主机

要添加多个主机，创建一个包含主机描述的YAML文件，在管理容器中创建YAML文件，然后运行ceph orch

```yaml
service_type: host
addr:
hostname: new-osd-1
labels:
  - mon
  - osd
  - mgr
---
service_type: host
addr:
hostname: new-osd-2
labels:
  - mon
  - osd
```

使用ceph orch apply添加OSD服务器

```bash
[ceph: root@adm ~]# ceph orch apply -i host.yaml
```

## 列出主机

ceph orch host ls可以列出所有主机，正常情况下STATUS是空的

```bash
[ceph: root@clienta ~]# ceph orch host ls 
HOST                     ADDR           LABELS  STATUS  
clienta.lab.example.com  172.25.250.10  _admin          
serverc.lab.example.com  172.25.250.12                  
serverd.lab.example.com  172.25.250.13                  
servere.lab.example.com  172.25.250.14                  
```

## 为OSD服务器配置额外的OSD存储

Ceph要求在考虑存储设备时需满足以下条件:

1. 设备不能有任何分区

2. 设备不能有LVM

3. 设备不能被挂载

4. 该设备不能包含文件系统

5. 设备不能包含Ceph BlueStore OSD

6. 设备大小必须大于5GB

ceph orch device ls可以列出集群中可用的osd，--wide选项可以查看更多详情

```bash
[ceph: root@clienta ~]#  ceph orch device ls --wide
Hostname                 Path      Type  Transport  RPM      Vendor  Model  Serial  Size   Health   Ident  Fault  Available  Reject Reasons                                                 
clienta.lab.example.com  /dev/vdb  hdd   Unknown    Unknown  0x1af4  N/A            10.7G  Unknown  N/A    N/A    Yes                                                                       
clienta.lab.example.com  /dev/vdc  hdd   Unknown    Unknown  0x1af4  N/A            10.7G  Unknown  N/A    N/A    Yes                                                                       
clienta.lab.example.com  /dev/vdd  hdd   Unknown    Unknown  0x1af4  N/A            10.7G  Unknown  N/A    N/A    Yes                                                                       
clienta.lab.example.com  /dev/vde  hdd   Unknown    Unknown  0x1af4  N/A            10.7G  Unknown  N/A    N/A    Yes                                                                       
clienta.lab.example.com  /dev/vdf  hdd   Unknown    Unknown  0x1af4  N/A            10.7G  Unknown  N/A    N/A    Yes                                                                       
serverc.lab.example.com  /dev/vde  hdd   Unknown    Unknown  0x1af4  N/A            10.7G  Unknown  N/A    N/A    Yes                                                                       
serverc.lab.example.com  /dev/vdf  hdd   Unknown    Unknown  0x1af4  N/A            10.7G  Unknown  N/A    N/A    Yes                                                                       
serverc.lab.example.com  /dev/vdb  hdd   Unknown    Unknown  0x1af4  N/A            10.7G  Unknown  N/A    N/A    No         Insufficient space (<10 extents) on vgs, LVM detected, locked  
serverc.lab.example.com  /dev/vdc  hdd   Unknown    Unknown  0x1af4  N/A            10.7G  Unknown  N/A    N/A    No         Insufficient space (<10 extents) on vgs, LVM detected, locked  
serverc.lab.example.com  /dev/vdd  hdd   Unknown    Unknown  0x1af4  N/A            10.7G  Unknown  N/A    N/A    No         Insufficient space (<10 extents) on vgs, LVM detected, locked  
serverd.lab.example.com  /dev/vde  hdd   Unknown    Unknown  0x1af4  N/A            10.7G  Unknown  N/A    N/A    Yes                                                                       
serverd.lab.example.com  /dev/vdf  hdd   Unknown    Unknown  0x1af4  N/A            10.7G  Unknown  N/A    N/A    Yes                                                                       
serverd.lab.example.com  /dev/vdb  hdd   Unknown    Unknown  0x1af4  N/A            10.7G  Unknown  N/A    N/A    No         Insufficient space (<10 extents) on vgs, LVM detected, locked  
serverd.lab.example.com  /dev/vdc  hdd   Unknown    Unknown  0x1af4  N/A            10.7G  Unknown  N/A    N/A    No         Insufficient space (<10 extents) on vgs, LVM detected, locked  
serverd.lab.example.com  /dev/vdd  hdd   Unknown    Unknown  0x1af4  N/A            10.7G  Unknown  N/A    N/A    No         Insufficient space (<10 extents) on vgs, LVM detected, locked  
servere.lab.example.com  /dev/vde  hdd   Unknown    Unknown  0x1af4  N/A            10.7G  Unknown  N/A    N/A    Yes                                                                       
servere.lab.example.com  /dev/vdf  hdd   Unknown    Unknown  0x1af4  N/A            10.7G  Unknown  N/A    N/A    Yes                                                                       
servere.lab.example.com  /dev/vdb  hdd   Unknown    Unknown  0x1af4  N/A            10.7G  Unknown  N/A    N/A    No         Insufficient space (<10 extents) on vgs, LVM detected, locked  
servere.lab.example.com  /dev/vdc  hdd   Unknown    Unknown  0x1af4  N/A            10.7G  Unknown  N/A    N/A    No         Insufficient space (<10 extents) on vgs, LVM detected, locked  
servere.lab.example.com  /dev/vdd  hdd   Unknown    Unknown  0x1af4  N/A            10.7G  Unknown  N/A    N/A    No         Insufficient space (<10 extents) on vgs, LVM detected, locked  
[ceph: root@clienta ~]# 
```

以root用户执行ceph orch daemon add osd命令，在指定主机上使用指定设备创建osd

```bash
[ceph: root@admin /]# ceph orch daemon add osd osd-1:/dev/vdb 
```

执行ceph orch apply osd --all-available-devices命令，在所有可用且未使用的设备上部署osd

```bash
[ceph: root@adm /]# ceph orch apply osd --all-available-devices
```

可以仅使用特定主机上的特定设备创建osd，下例中，在每台主机上由default_drive_group组中提供的后端设备/dev/vdc和/dev/vdd创建两个osd

```yaml
[ceph: root@adm I]# cat lvarlliblcephlosdlosd_spec . yml 
service_type: osd
service_id: default_drive_group
placement:
  hosts:
    - osd-1
    - osd-2
data_devices:
  paths:
    - /dev/vdc
    - /dev/vdd
```

执行ceph orch apply命令实现YAML文件中的配置

```bash
[ceph: root@adm /]# ceph orch apply -i /var/lib/ceph/osd/osd_spec.yml
```


